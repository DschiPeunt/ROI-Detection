\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[superscript]{cite}
\usepackage{nicefrac}
\usepackage{upgreek}
\usepackage{paralist}
\usepackage{stmaryrd}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{qtree}
\usepackage{dsfont}
\usepackage{eurosym}
\usepackage{setspace}
\usepackage{fancyhdr}
% \usepackage[colorlinks=true,linkcolor=blue]{hyperref}				% Blaue Links sehen meiner Ansicht nach besser aus als die rot umrandeten Verweise

% Kopfzeile
\newcommand\shorttitle{}
\newcommand\authors{Dominik Blank}

\fancyhf{} % sets all head and foot elements empty
\fancyhead[L]{\shorttitle}
\fancyhead[R]{\authors}
\pagestyle{fancy} % sets the page style to the style delivered and editable with fancyhdr

% Kommandos, Operatoren, etc.
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\DeclareMathOperator{\thetafunc}{\uptheta}

% Mathe-Umgebungen
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% ENDE PRÄAMBEL

\author{Dominik Blank}
\title{Gleichmäßige obere Schranken beim Kreisproblem}

\onehalfspacing
\setlength{\parindent}{0pt}
\allowdisplaybreaks

\begin{document}
%\begin{titlepage}
%
%\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
%
%\center % Center everything on the page
% 
%%----------------------------------------------------------------------------------------
%%	HEADING SECTIONS
%%----------------------------------------------------------------------------------------
%
%\textsc{\Large Georg-August-Universität Göttingen}\\[1.5cm] % Name of your university/college
%\textsc{\large Bachelor-Arbeit zum Thema}\\[0.5cm] % Major heading such as course name
%% \textsc{\large Minor Heading}\\[0.5cm] % Minor heading such as course title
%
%%----------------------------------------------------------------------------------------
%%	TITLE SECTION
%%----------------------------------------------------------------------------------------
%
%\HRule \\[0.4cm]
%{\large \bfseries Gleichmäßige obere Schranken beim Kreisproblem}\\[0.2cm] % Title of your document
%\HRule \\[1.5cm]
% 
%%----------------------------------------------------------------------------------------
%%	AUTHOR SECTION
%%----------------------------------------------------------------------------------------
%
%\begin{minipage}{0.4\textwidth}
%\begin{flushleft} \large
%\emph{Autor:}\\
%Dominik \textsc{Blank} % Your name
%\end{flushleft}
%\end{minipage}
%~
%\begin{minipage}{0.5\textwidth}
%\begin{flushright} \large
%\emph{Betreuer:} \\
%Prof. Dr. Valentin \textsc{Blomer} % Supervisor's Name
%\end{flushright}
%\end{minipage}\\[4cm]
%
%%----------------------------------------------------------------------------------------
%%	DATE SECTION
%%----------------------------------------------------------------------------------------
%
%{\large Göttingen, den \today}\\[3cm] % Date, change the \today to a set date if you want to be precise
%
%\begin{abstract}
%	In dieser Arbeit wird eine asymptotische Formel für die ganzzahligen Darstellungen der natürlichen Zahlen $m \leq R$ durch eine positiv-definite quadratische Form hergeleitet.
%	Die implizite Konstante des Fehlerterms wird dabei nicht von der jeweiligen quadratischen Form abhängen.
%\end{abstract}
%\end{titlepage}

\newpage

\section{Testing for a rectangular region of interest}

\subsection{Definitions}

\begin{definition}
	Let $M, N \in \mathbb{N}$ and $V \in \mathbb{R}^{M \times N}$ be a matrix. Assume there are two pairs of indices $(i_1, j_1), (i_2, j_2)$ with $1 \leq i_1 \leq i_2 \leq M$ and $1 \leq j_1 \leq j_2 \leq N$, such that
	\begin{equation}
		V(i, j) \neq 0 \textrm{ if and only if } (i, j) \in \{ i_1, \dots, i_2 \} \times \{ j_1, \dots, j_2 \}
	\end{equation}
	We call $R = \{ i_1, \dots, i_2 \} \times \{ j_1, \dots, j_2 \}$ a \emph{rectangular region of interest (rROI)} and say that $V$ contains the rROI $R$.
	
	Furthermore, we call $(i_1, j_1)$ the \emph{top left corner} and $(i_2, j_2)$ the \emph{bottom right corner} of $R$.
\end{definition}

\begin{definition}
	Let $M, N \in \mathbb{N}$ and $c \in \mathbb{R} \setminus \{ 0 \}$. Let $V \in \mathbb{R}^{M \times N}$ be a matrix, that only takes values in the set $\{ 0, \pm c \}$ and that contains a rectangular region of interest $R$. We say that $R$ has a \emph{checkerboard pattern}, if one of the following relations is true:
	\begin{subequations}
		\begin{align}
			\textrm{For all } (i, j) \in R: V(i, j) = c &\textrm{ if and only if } i + j \textrm{ is odd}.\\
			\textrm{For all } (i, j) \in R: V(i, j) = c &\textrm{ if and only if } i + j \textrm{ is even}.
		\end{align}
	\end{subequations}
\end{definition}

\begin{remark}
	If the first relation in the definition is true, that immediately implies, that
	\begin{equation*}
		\textrm{for all } (i, j) \in R: V(i, j) = - c \textrm{ if and only if } i + j \textrm{ is even},
	\end{equation*}
	since $V$ takes only values in $\{ 0, \pm c \}$, but for $(i, j) \in R$ we have $V(i,j) \neq 0$.
	Similarly, if the second relation is true, it implies that
	\begin{equation*}
		\textrm{for all } (i, j) \in R: V(i, j) = - c \textrm{ if and only if } i + j \textrm{ is odd}.
	\end{equation*}
	In both cases the values of $V$ alternate between $+c$ and $-c$ along the rows and columns of $R$. This is similar to the classical checkerboard pattern.
\end{remark}

\begin{definition}
	Let $M, N \in \mathbb{N}$ and $c \in \mathbb{R} \setminus \{ 0 \}$. We define $\mathcal{V}_c^{M, N}$ to be the set of all matrices $V \in \mathbb{R}^{M \times N}$, that only take values in the set $\{ 0, \pm c \}$ and that contain a rectangular region of interest with a checkerboard pattern.
\end{definition}

\newpage

\subsection{Statistical model}

Let $M, N \in \mathbb{N}$, $c \in \mathbb{R} \setminus \{ 0 \}$ and $G = \left\{ 1, \dots, M \right\} \times \left\{ 1, \dots, N \right\}$. Assume we are given noisy data
\begin{equation}\label{image}
	F(i, j) = c + V(i, j) + \varepsilon_{i, j}
\end{equation}
where $(i, j) \in G$, $V \in \mathcal{V}_c^{M, N}$ and $\varepsilon_{i, j} \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. normal distributed random variables for some $\sigma > 0$ and for all $(i, j) \in G$.

Although defined as a matrix, we often refer to $F$ and $V$ as images and to $(i, j)$ as a pixel. Let $R$ be the rectangular region of interest contained in $V$ and let $(i_1, j_1)$ and $(i_2, j_2)$ be the top left and bottom right corner of $R$, respectively. We aim to find a statistical test to determine for each individual pixel whether $(i, j) \in R$ or $(i, j) \notin R$.

To proceed we define for each pixel $(i, j) \in G$ the four values
\begin{align}
	D^\pm_1(i, j) &= V(i \pm 1, j) - V(i, j) \label{D1} \\
	D^\pm_2(i, j) &= V(i, j \pm 1) - V(i, j) \label{D2}
\end{align}
where we set
\begin{align*}
	V(i, 0) &= V(i, N) \\
	V(0, j) &= V(M, j) \\
	V(i, N+1) &= V(i, 1) \\
	V(M+1, j) &= V(1, j)
\end{align*}
to adjust to boundary issues. We now combine these values into two new values and assign to each pair $(i, j) \in G$ the values
\begin{equation}\label{D}
	D^\pm(i, j) = \sqrt{D_1^\pm(i, j)^2 + D_2^\pm(i, j)^2}
\end{equation}

Since we have assumed that $V \in \mathcal{V}_c^{M, N}$, we know that $V(i, j) = 0$ if and only if $(i, j) \notin R$. Now if $(i, j) \notin R$ it follows that $i \notin \{ i_1, \dots, i_2 \}$ or $j \notin \{ j_1, \dots, j_2 \}$. We have to distinguish four cases here:
\begin{align*}
	i < i_1 &\Rightarrow (i, j - 1) \notin R \textrm{ and } (i - 1, j) \notin R \\
	&\Rightarrow V(i, j - 1) = V(i - 1, j) = 0 \\
	j < j_1 &\Rightarrow (i - 1, j) \notin R \textrm{ and } (i, j - 1) \notin R \\
	&\Rightarrow V(i - 1, j) = V(i, j - 1) = 0 \\
	i > i_2 &\Rightarrow (i, j + 1) \notin R \textrm{ and } (i + 1, j) \notin R \\
	&\Rightarrow V(i, j + 1) = V(i + 1, j) = 0 \\
	j > j_2 &\Rightarrow (i + 1, j) \notin R \textrm{ and } (i, j + 1) \notin R \\
	&\Rightarrow V(i + 1, j) = V(i, j + 1) = 0
\end{align*}

We see, that in the first two cases, we have $D^-_1(i, j) = D^-_2(i, j) = 0$, which yields $D^-(i, j) = 0$. In the latter two cases, we get $D^+_1(i, j) = D^+_2(i, j) = 0$ and thus $D^+(i, j) = 0$. Thus, if $(i, j) \notin R$ it follows that $\min \{ D^+(i, j), D^-(i, j) \} = 0$.

On the other hand, we have assumed, that $R$ has a checkerboard pattern, thus $D^\pm(i, j) \neq 0$ for $(i, j) \in R$. This yields the equivalence
\begin{equation*}
	(i, j) \notin R \Leftrightarrow \min \{ D^+(i, j), D^-(i, j) \} = 0
\end{equation*}

Since our goal is to test for $(i, j) \in R$, we define a null hypothesis for each individual pixel:
\begin{equation}
	H_0(i, j): \min \{ D^+(i, j), D^-(i, j) \} = 0
\end{equation}

Unfortunately, we do not the actual values of $V$, which makes $D^+$ and $D^-$ non-observable. We are given noisy data though. Based on the noisy data, we define four observable values for each pixel
\begin{align}
	\tilde{D}^\pm_1(i, j) &= F(i \pm 1, j) - F(i, j) \label{D1tilde} \\
	\tilde{D}^\pm_2(i, j) &= F(i, j \pm 1) - F(i, j) \label{D2tilde}
\end{align}
where we again define
\begin{align*}
	F(i, 0) &= F(i, N) \\
	F(0, j) &= F(M, j) \\
	F(i, N+1) &= F(i, 1) \\
	F(M+1, j) &= F(1, j)
\end{align*}
to adjust to boundary issues. Again we combine these values into two new values
\begin{equation}\label{Dtilde}
	\tilde{D}^\pm(i, j) = \sqrt{\tilde{D}_1^\pm(i, j)^2 + \tilde{D}_2^\pm(i, j)^2}
\end{equation}

We use these values for our test statistic and test for each individual pixel $(i, j)$ the null hypothesis
\begin{equation}
	H_0(i, j): \min \{ D^+(i, j), D^-(i, j) \} = 0
\end{equation}
against the alternative hypothesis
\begin{equation}
	H_1(i, j): \min \{ D^+(i, j), D^-(i, j) \} \neq 0
\end{equation}
using the test statistic
\begin{equation}
	T(i, j) = \min \{ \tilde{D}^+(i, j), \tilde{D}^-(i, j) \}
\end{equation}

\newpage

After having established our hypotheses and test statistic, we want to show, that we can ensure a given statistical significance $\alpha$. This will be done in two steps: In a nutshell, the testing procedure is the combination of two testing procedures ($D^+$ and $D^-$) and we will see, that we can bound our combination by either of the individual ones. In a second step, we will bound the individual procedures by computing their cumulative distribution function.

If the null hypothesis for a given pixel $(i, j)$ is true, we can distiguish the two cases: $D^+(i, j) = 0$ or $D^-(i, j) = 0$. Note that these two cases are not exclusive. Without loss of generality we assume the first case, i.e. $D^+(i, j) = 0$. For $t \in \mathbb{R}^+$ we then have
\begin{equation}\label{typeIbound}
	\begin{aligned}
		\mathbb{P}(T(i, j) \geq t \mid H_0(i, j)) &= \mathbb{P}(\min \{ \tilde{D}^+(i, j), \tilde{D}^-(i, j) \} \geq t \mid D^+(i, j) = 0) \\
		&= \mathbb{P}(\{ \tilde{D}^+(i, j) \geq t \} \cap \{ \tilde{D}^-(i, j) \geq t \} \mid D^+(i, j) = 0) \\
		&\leq \mathbb{P}(\tilde{D}^+(i, j) \geq t \mid D^+(i, j) = 0)
	\end{aligned}
\end{equation}

In the second case we can actually get the same inequality by using the fact that $\mathbb{P}(\tilde{D}^-(i, j) \geq t \mid D^-(i, j) = 0) = \mathbb{P}(\tilde{D}^+(i, j) \geq t \mid D^+(i, j) = 0)$.

We want to take a look at the distribution of $\tilde{D}^+(i, j)$ given $D^+(i, j) = 0$:
\begin{equation}
	\begin{aligned}
		p &\coloneqq \mathbb{P}(\tilde{D}^+(i, j) \leq t \mid D^+(i, j) = 0) \\
		&= \mathbb{P}( (c + V(i + 1, j) + \varepsilon_{i + 1, j} - c - V(i, j) - \varepsilon_{i, j})^2 \\
		&+ (c + V(i, j + 1) + \varepsilon_{i, j + 1} - c - V(i, j) - \varepsilon_{i, j})^2 \leq t^2 \mid D^\pm(i, j) = 0) \\
		&= \mathbb{P}\left( (\varepsilon_{i + 1, j} - \varepsilon_{i, j})^2 + (\varepsilon_{i, j + 1} - \varepsilon_{i, j})^2 \leq t^2 \right)
	\end{aligned}
\end{equation}

Assuming the common term $\varepsilon_{i, j}$ to be constant and by defining
\begin{align*}
	X_1 &= \varepsilon_{i + 1, j} - \varepsilon_{i, j} \sim \mathcal{N}(\varepsilon_{i, j}, \sigma^2) \\
	X_2 &= \varepsilon_{i, j + 1} - \varepsilon_{i, j} \sim \mathcal{N}(\varepsilon_{i, j}, \sigma^2)
\end{align*}
we obtain
\begin{equation}
	p = \mathbb{P}\left( \sqrt{\left( \frac{X_1}{\sigma} \right)^2 + \left( \frac{X_2}{\sigma} \right)^2} \leq \frac{t}{\sigma} \right)
\end{equation}

This shows, that the square root inside has a non-central Chi distribution with two degrees of freedom and non-centrality parameter
\begin{equation}
	\lambda = \sqrt{\left( \frac{\varepsilon_{i, j}}{\sigma} \right)^2 + \left( \frac{\varepsilon_{i, j}}{\sigma} \right)^2} = \frac{\sqrt{2} \abs{\varepsilon_{i, j}}}{\sigma}
\end{equation}
which shows that
\begin{equation}
	\sqrt{\left( \frac{X_1}{\sigma} \right)^2 + \left( \frac{X_2}{\sigma} \right)^2} \sim \chi_2 \left( \frac{\sqrt{2} \abs{\varepsilon_{i, j}}}{\sigma} \right)
\end{equation}

Up until this point we assumed $\varepsilon_{i, j}$ to be constant, but it is a normal distributed random variable with zero mean and standard deviation $\sigma$. Thus, we have a compound probability distribution:
\begin{align*}
	p &= \mathbb{P}\left( \sqrt{\left( \frac{X_1}{\sigma} \right)^2 + \left( \frac{X_2}{\sigma} \right)^2} \leq \frac{t}{\sigma} \right) \\
	&= \int_0^\frac{t}{\sigma} \int_0^\infty \underbrace{x \exp \left( - \frac{x^2}{2} - \frac{\eta^2}{\sigma^2} \right) I_0 \left( \frac{\sqrt{2} \eta}{\sigma} x \right)}_{\textrm{pdf of} \ \chi_2 \left( \frac{\sqrt{2} \eta}{\sigma} \right) \ \textrm{for fixed} \ \eta} \underbrace{\frac{2}{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{\eta^2}{2 \sigma^2} \right)}_{\textrm{pdf of absolute value of} \ \mathcal{N}(0, \sigma^2)} d\eta dx \\
	&= \frac{2}{\sqrt{2 \pi \sigma^2}} \int_0^\frac{t}{\sigma} x \exp \left( - \frac{x^2}{2} \right) \int_0^\infty \exp \left( - \frac{3}{2 \sigma^2} \eta^2 \right) I_0 \left( \frac{\sqrt{2} x}{\sigma} \eta \right) d\eta dx
\end{align*}

where $I_0$ is the modified Bessel function of the first kind. We can solve the inner integral first. For $\mathrm{Re} \ \nu > -1$, $\mathrm{Re} \ \alpha > 0$ the following equality holds CITE!!!:
\begin{equation*}
	\int_0^\infty \exp \left( - \alpha x^2 \right) I_\nu ( \beta x ) dx = \frac{\sqrt{\pi}}{2 \sqrt{\alpha}} \exp \left( \frac{\beta^2}{8 \alpha} \right) I_{\frac{1}{2} \nu} \left( \frac{\beta^2}{8 \alpha} \right)
\end{equation*}

In our case we have $\nu = 0$, $\alpha = \frac{3}{2 \sigma^2}$ and $\beta = \frac{\sqrt{2} x}{\sigma}$, which yields
\begin{align*}
	\int_0^\infty \exp \left( - \frac{3}{2 \sigma^2} \eta^2 \right) I_0 \left( \frac{\sqrt{2} x}{\sigma} \eta \right) d\eta &= \frac{\sqrt{\pi}}{2 \sqrt{\frac{3}{2 \sigma^2}}} \exp \left( \frac{\frac{2 x^2}{\sigma^2}}{8 \frac{3}{2 \sigma^2}} \right) I_0 \left( \frac{\frac{2 x^2}{\sigma^2}}{8 \frac{3}{2 \sigma^2}} \right) \\
	&= \frac{\sqrt{\pi} \sigma}{\sqrt{6}} \exp \left( \frac{x^2}{6} \right) I_0 \left( \frac{x^2}{6} \right)
\end{align*}

Plugging this in, we get
\begin{align*}
	p &= \frac{2}{\sqrt{2 \pi \sigma^2}} \int_0^\frac{t}{\sigma} x \exp \left( - \frac{x^2}{2} \right) \frac{\sqrt{\pi} \sigma}{\sqrt{6}} \exp \left( \frac{x^2}{6} \right) I_0 \left( \frac{x^2}{6} \right) dx \\
	&= \frac{1}{\sqrt{3}} \int_0^\frac{t}{\sigma} x \exp \left( - \frac{x^2}{2} \right) \exp \left( \frac{x^2}{6} \right) I_0 \left( \frac{x^2}{6} \right) dx \\
	&= \frac{1}{\sqrt{3}} \int_0^\frac{t}{\sigma} x \exp \left( - \frac{x^2}{3} \right) I_0 \left( \frac{x^2}{6} \right) dx
\end{align*}

To proceed, we need to integrate by parts to replace the modified Bessel function $I_0$ with order zero by a modified Bessel function $I_1$ with order one.
\begin{align*}
	p &= \frac{1}{\sqrt{3}} \int_0^\frac{t}{\sigma} x \exp \left( - \frac{x^2}{3} \right) I_0 \left( \frac{x^2}{6} \right) dx \\
	&= \frac{1}{\sqrt{3}} \left[ - \frac{3}{2} \exp \left( - \frac{x^2}{3} \right) I_0 \left( \frac{x^2}{6} \right) \right]_0^\frac{t}{\sigma} + \frac{1}{2 \sqrt{3}} \int_0^\frac{t}{\sigma} \exp \left( - \frac{x^2}{3} \right) x I_1 \left( \frac{x^2}{6} \right) dx \\
	&= \frac{1}{\sqrt{3}} \left( \frac{3}{2} - \frac{3}{2} \exp \left( - \frac{t^2}{3 \sigma^2} \right) I_0 \left( \frac{t^2}{6 \sigma^2} \right) \right) + \frac{1}{2 \sqrt{3}} \int_0^\frac{t}{\sigma} \exp \left( - \frac{x^2}{3} \right) x I_1 \left( \frac{x^2}{6} \right) dx
\end{align*}

In the next step we substitute $y = x^2$ in the remaining integral, which leaves us with
\begin{equation*}
	p = \frac{1}{\sqrt{3}} \left( \frac{3}{2} - \frac{3}{2} \exp \left( - \frac{t^2}{3 \sigma^2} \right) I_0 \left( \frac{t^2}{6 \sigma^2} \right) \right) + \frac{1}{4 \sqrt{3}} \int_0^\frac{t}{\sigma} \exp \left( - \frac{y}{3} \right) I_1 \left( \frac{y}{6} \right) dy
\end{equation*}

We want to solve the remaining integral. Let $p \neq b$ and $s = \sqrt{p^2 - b^2}$, $u = \sqrt{a (p - s)}$ and $v = \sqrt{a (p + s)}$. Then CITE!!!
\begin{equation*}
	\int_0^a \exp(-p x) I_M ( b x ) dx = \frac{1}{s b^M} \left( (p - s)^M ( 1 - Q_M(u, v) ) - (p + s)^M ( 1 - Q_M(v, u) ) \right)
\end{equation*}
where $Q_M$ denotes the Marcum $Q$-function. The Marcum $Q$-function is only defined for $M \geq 1$, which made the integration by parts necessary. Applying this equation with $M = 1$ to the integral yields
\begin{align*}
	\int_0^\frac{t}{\sigma} \exp \left( - \frac{y}{3} \right) I_1 \left( \frac{y}{6} \right) dy &= \frac{1}{\frac{1}{2 \sqrt{3}} \frac{1}{6}} \frac{2 - \sqrt{3}}{6} \left( 1 - Q_1 \left( \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right) \right) \\
	&- \frac{1}{\frac{1}{2 \sqrt{3}} \frac{1}{6}} \frac{2 + \sqrt{3}}{6} \left( 1 - Q_1 \left( \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right) \right) \\
	&= 2 \sqrt{3} (2 - \sqrt{3}) \left( 1 - Q_1 \left( \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right) \right) \\
	&- 2 \sqrt{3} (2 + \sqrt{3}) \left( 1 - Q_1 \left( \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right) \right)
\end{align*}

Plugging this in, we obtain the final result
\begin{align*}
	p &= \frac{1}{\sqrt{3}} \left( \frac{3}{2} - \frac{3}{2} \exp \left( - \frac{t^2}{3 \sigma^2} \right) I_0 \left( \frac{t^2}{6 \sigma^2} \right) \right) + \frac{1}{4 \sqrt{3}} \int_0^\frac{t}{\sigma} \exp \left( - \frac{y}{3} \right) I_1 \left( \frac{y}{6} \right) dy \\
	&= \frac{1}{\sqrt{3}} \left( \frac{3}{2} - \frac{3}{2} \exp \left( - \frac{t^2}{3 \sigma^2} \right) I_0 \left( \frac{t^2}{6 \sigma^2} \right) \right) \\
	&+ \frac{2 \sqrt{3}}{4 \sqrt{3}} (2 - \sqrt{3}) \left( 1 - Q_1 \left( \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right) \right) \\
	&- \frac{2 \sqrt{3}}{4 \sqrt{3}} (2 + \sqrt{3}) \left( 1 - Q_1 \left( \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right) \right) \\
	&= \frac{1}{\sqrt{3}} \left( \frac{3}{2} - \frac{3}{2} \exp \left( - \frac{t^2}{3 \sigma^2} \right) I_0 \left( \frac{t^2}{6 \sigma^2} \right) \right) - \sqrt{3} \\
	&- \frac{2 - \sqrt{3}}{2} Q_1 \left( \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right) \\
	&+ \frac{2 + \sqrt{3}}{2} Q_1 \left( \frac{2 + \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}}, \frac{2 - \sqrt{3}}{6} \sqrt{\frac{t}{\sigma}} \right)
\end{align*}

Thus, we have computed the distribution of $\tilde{D}(i, j)$ given $D^+(i, j) = 0$. It is obvious, that there is no easy way to compute an inverse function of this cumulative distribution function, but for a given $\alpha$ we can find numerical solutions to ensure $\mathbb{P}(\tilde{D}^+(i, j) \leq t \mid D^+(i, j) = 0) \geq 1 - \alpha$. By doing so, we get
\begin{equation}
	\mathbb{P}(T(i, j) \geq t \mid H_0(i, j)) \leq \mathbb{P}(\tilde{D}^+(i, j) \geq t \mid D^+(i, j) = 0) \leq \alpha
\end{equation}
and can thus ensure a statistical significance of $\alpha$.

\newpage

In equation \ref{typeIbound} we have managed to bound the probability of a type I error in our testing procedure by the probability of a type I error when testing for $D^+(i, j) = 0$ using the test statistic $\tilde{D}^+(i, j)$. We want to do the same for the probability of a type II error.

\newpage

\begin{theorem}
	Assume the following statistical model:
	
	Let $M, N \in \mathbb{N}$ and $G = \left\{ 1, \dots, M \right\} \times  \left\{ 1, \dots, N \right\}$. We are given data
	\begin{equation}\label{f}
		F(i, j) = c + V(i, j) + \varepsilon_{i, j}
	\end{equation}
	where $(i, j) \in G$, $c \in \mathbb{R}$ is constant, $V(i, j) \in \{ 0, \pm c \}$ and $\varepsilon_{i, j} \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. normal distributed random variables for some $\sigma > 0$ and for all $(i, j) \in G$.
	
	Assume that $V$ contains a rectangular region of interest $R$ and that $R$ has a checkerboard pattern.
	
	Let $\varphi_\alpha \in \{ 0, 1 \}^{M \times N}$ be the binary matrix, that represents the decision of the testing procedure to a given statistical significance $\alpha$.
	
	Let $k \in \mathbb{N}$ be odd and $B$ be a square structuring element with side length $k$.
	
	Then the following inequality holds:
	\begin{equation*}
		\mathbb{P}((\varphi_\alpha \circ B)(i, j) = 1 \mid H_0(i, j)) \leq k \alpha^{\frac{k+1}{2}}
	\end{equation*}
\end{theorem}
\begin{proof}
	We aim to find an upper bound for the probability
	\begin{equation*}
		\mathbb{P}((\varphi_\alpha \circ B)(i, j) = 1 \mid H_0(i, j))
	\end{equation*}
	To do this, we first notice that $H_0(i, j)$ is equivalent to $V(i, j) = 0$, but since $V$ contains a rectangular region of interest, this means that $i < i_1$ or $i > i_2$ or $j < j_1$ or $j > j_2$. We need to differentiate cases here. These four cases are not mutually exclusive, but have different implications for the row/column of the index $(i, j)$ and a neighbouring row/column:\\
	
	\begin{tabular}{|c|c|c|}
		\hline
		case & row/column of $(i, j)$ & neighbouring row/column \\
		\hline
		$i < i_1$ & $V(i, 1) = \dots = V(i, N) = 0$ & $V(i-1, 1) = \dots = V(i-1, N) = 0$ \\
		\hline
		$i > i_2$ & $V(i, 1) = \dots = V(i, N) = 0$ & $V(i+1, 1) = \dots = V(i+1, N) = 0$ \\
		\hline
		$j < j_1$ & $V(1, j) = \dots = V(M, j) = 0$ & $V(1, j-1) = \dots = V(M, j-1) = 0$ \\
		\hline
		$j > j_2$ & $V(1, j) = \dots = V(M, j) = 0$ & $V(1, j+1) = \dots = V(M, j+1) = 0$ \\
		\hline
	\end{tabular}\\
	
	Without loss of generality we assume the first case. This means that the null hypotheses $H(i, 1), \dots, H(i, N)$ and $H(i-1, 1), \dots, H(i-1, N)$ are true.
	
	To be even more precise, it implies that $D^-(i, 1) = \dots = D^-(i, N) = 0$.
	
	We have assumed the side length $k$ of the structuring element $B$ to be odd.
	
	We define the two index sets $K = \{ -\frac{k - 1}{2}, -\frac{k - 3}{2}, \dots, \frac{k - 3}{2}, \frac{k - 1}{2} \}$ and $K_1 = \{ -\frac{k - 1}{2}, -\frac{k - 5}{2}, \dots, \frac{k - 5}{2}, \frac{k - 1}{2} \}$. This yields
	\begin{align*}
		\mathbb{P}&((\varphi_\alpha \circ B)(i, j) = 1 \mid H_0(i, j)) \\
		&= \mathbb{P} \left( \bigcup_{\tilde{m}, \tilde{n} \in K} \bigcap_{m, n \in K} \{ \varphi_\alpha(i + m - \tilde{m}, j + n - \tilde{n}) = 1 \} \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&= \sum_{\tilde{n} \in K} \mathbb{P} \left( \bigcup_{\tilde{m} \in K} \bigcap_{m, n \in K} \{ \varphi_\alpha(i + m - \tilde{m}, j + n - \tilde{n}) = 1 \} \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&\leq \sum_{\tilde{n} \in K} \mathbb{P} \left( \bigcup_{\tilde{m} \in K} \bigcap_{n \in K, m = \tilde{m}} \{ \varphi_\alpha(i + m - \tilde{m}, j + n - \tilde{n}) = 1 \} \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&= \sum_{\tilde{n} \in K} \mathbb{P} \left( \bigcup_{\tilde{m} \in K} \bigcap_{n \in K} \{ \varphi_\alpha(i, j + n - \tilde{n}) = 1 \} \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&= \sum_{\tilde{n} \in K} \mathbb{P} \left( \bigcap_{n \in K} \{ \varphi_\alpha(i, j + n - \tilde{n}) = 1 \} \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&\leq \sum_{\tilde{n} \in K} \mathbb{P} \left( \bigcap_{n \in K_1} \{ \varphi_\alpha(i, j + n - \tilde{n}) = 1 \} \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&\leq \sum_{\tilde{n} \in K} \prod_{n \in K_1} \mathbb{P} \left( \{ \varphi_\alpha(i, j + n - \tilde{n}) = 1 \} \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&= \sum_{\tilde{n} \in K} \prod_{n \in K_1} \mathbb{P} \left( T(i, j + n - \tilde{n}) \geq t_\alpha \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&\leq \sum_{\tilde{n} \in K} \prod_{n \in K_1} \mathbb{P} \left( \tilde{D}^-(i, j + n - \tilde{n}) \geq t_\alpha \mid D^-(i, 1) = \dots = D^-(i, N) = 0 \right) \\
		&= \sum_{\tilde{n} \in K} \prod_{n \in K_1} \mathbb{P} \left( \tilde{D}^-(i, j + n - \tilde{n}) \geq t_\alpha \mid D^-(i, j + n - \tilde{n}) = 0 \right) \\
		&\leq \sum_{\tilde{n} \in K} \prod_{n \in K_1} \alpha \\
		&= \abs{K} \alpha^{\abs{K_1}} \\
		&= k \alpha^{\frac{k+1}{2}}
	\end{align*}
	
	The other three cases can be proven in a similar way by swapping the roles of $m$ or $\tilde{m}$ with $n$ or $\tilde{n}$, respectively and/or by replacing $D^-$ by $D^+$.
\end{proof}


\end{document}
